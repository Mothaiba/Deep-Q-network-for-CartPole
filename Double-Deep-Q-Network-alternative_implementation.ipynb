{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d891a860-1c02-4ea0-83c9-47ecdfab1fb7",
   "metadata": {},
   "source": [
    "# Importation and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a3d4224-7462-4bb3-be12-cee1c213c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5a0de1-d13a-465b-912d-5e95e9ef98ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c744810-77c9-4afb-bb04-0a960d47a0b8",
   "metadata": {},
   "source": [
    "# CartPole environment description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce04fef-24f5-455b-8a20-fda9faf61269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n",
      "Observation (or state) space: Box(-3.4028234663852886e+38, 3.4028234663852886e+38, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "print('Action space:', env.action_space)\n",
    "print('Observation (or state) space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27763ebd-bf1c-4526-9fec-8e222b7d3338",
   "metadata": {},
   "source": [
    "According to [OpenAI gym wiki](https://github.com/openai/gym/wiki/CartPole-v0):\n",
    "\n",
    "There are 2 actions we can take:\n",
    "- Move to the left (denoted as `0`).\n",
    "- Move to the right (denoted as `1`).\n",
    "\n",
    "An observation contains a list of 4 floating-point numbers, they are, respectively:\n",
    "- Cart position.\n",
    "- Cart velocity.\n",
    "- Pole angle.\n",
    "- Pole velocity at tip.\n",
    "\n",
    "Common APIs of the game:\n",
    "- env.seed(seed: int) -> None. Set random seed.\n",
    "- env.reset() -> None. Reset the game, used when we get a game-over.\n",
    "- env.action_space.sample() -> int (an action id). Get a random valid action.\n",
    "- env.step(action: int) -> None. Simulate this action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080822a-34b2-4d15-bbdb-8f2f1b26eed7",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f91dc8f-02b7-40cb-b47b-cf8b62dc7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.8 # the discount factor of future reward\n",
    "TARGET_LEARNING_RATE = 0.01\n",
    "\n",
    "MIN_MEMORY_SIZE = 1000\n",
    "MAX_MEMORY_SIZE = 5000\n",
    "SIMULATIONS_PER_TRAINING = 4 # at each step, we simulate this number of experiences before training a minibatch\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "TRAIN_ROUNDS = 300 # a round means a game\n",
    "\n",
    "# higher epsilon means more exploration.\n",
    "# epsilon is decreased gradually.\n",
    "def get_epsilon(curr_round: int, train_rounds: int):\n",
    "    return 1 - curr_round/train_rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc952665-352d-4fa3-814f-40a75a39ece3",
   "metadata": {},
   "source": [
    "# Class and function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "249dbc69-7974-4b94-a58b-577cc4505640",
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', 'observation action reward new_observation done')\n",
    "\n",
    "# Experience Replay\n",
    "class ExperienceMemory():\n",
    "    def __init__(self, memory_capacity):\n",
    "        self.memory = deque(maxlen=memory_capacity)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def push(self, e: Experience):\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self, size):\n",
    "        return random.sample(self.memory, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bca98b85-29fc-4e13-9f78-935714b31a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, observation_shape, action_shape, lr):\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(20, activation='relu', input_shape=observation_shape),\n",
    "            tf.keras.layers.Dense(10, activation='relu'),\n",
    "            tf.keras.layers.Dense(action_shape),\n",
    "        ])\n",
    "        \n",
    "        self.model.compile(loss=tf.keras.losses.Huber(), optimizer=tf.optimizers.Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    \n",
    "    def forward(self, batch_observations):\n",
    "        return self.model.predict(batch_observations)\n",
    "            \n",
    "    def train(self, batch_x, batch_y):\n",
    "        self.model.train_on_batch(batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f6684e-a6e9-49e4-a10b-984709fe1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_experience(env, model, eps, curr_observation):\n",
    "    # select the next action (from the current state)\n",
    "    if random.random() < eps:\n",
    "        # use a random state\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        # use the best action (according to the current model)\n",
    "        action_scores = model.forward(np.expand_dims(curr_observation, 0)).flatten()\n",
    "        action = np.argmax(action_scores)\n",
    "\n",
    "    # simulate this action and push the experience to experience_memory\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    return Experience(curr_observation, action, reward, new_observation, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae09dcbc-3409-441b-8423-69c95c4d0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, target_model, minibatch):\n",
    "    # batch_curr_observations shape: [batch size, observation shape]\n",
    "    batch_curr_observations = np.array([e.observation for e in minibatch])\n",
    "    # batch_curr_qualities shape: [batch size, action size]\n",
    "    batch_curr_qualities = model.forward(batch_curr_observations)\n",
    "    \n",
    "    # batch_new_observations shape: [batch size, observation shape]\n",
    "    batch_new_observations = np.array([e.new_observation for e in minibatch])\n",
    "    # batch_q_new_qualities shape: [batch size, action size]\n",
    "    # qualities of the actions on new observations as evaluated by the main network\n",
    "    batch_q_new_qualities = model.forward(batch_new_observations)\n",
    "    # batch_best_actions shape: [batch size]\n",
    "    # best action for each state as evaluated by the main network\n",
    "    batch_best_actions = tf.argmax(batch_q_new_qualities, axis=1)\n",
    "    # batch_t_new_qualities shape: [batch size, action size]\n",
    "    # qualities of the actions on new observations as evaluated by the target network\n",
    "    batch_t_new_qualities = target_model.forward(batch_new_observations)\n",
    "    \n",
    "    batch_x, batch_y = [], []\n",
    "    for id, e in enumerate(minibatch):\n",
    "        observation = e.observation\n",
    "        action = e.action\n",
    "        reward = e.reward\n",
    "        new_observation = e.new_observation\n",
    "        done = e.done\n",
    "        \n",
    "        # curr_qualities shape: [action size]\n",
    "        curr_qualities = batch_curr_qualities[id]\n",
    "        # new_qualities shape: [action size]\n",
    "        new_qualities = batch_t_new_qualities[id]\n",
    "        selected_quality : float = new_qualities[batch_best_actions[id]]\n",
    "        if done:\n",
    "            updated_quality : float = reward\n",
    "        else:\n",
    "            updated_quality : float = reward + GAMMA*selected_quality\n",
    "        # desired_qualities shape: [action size]\n",
    "        desired_qualities = curr_qualities\n",
    "        desired_qualities[action] = (1 - TARGET_LEARNING_RATE)*curr_qualities[action] + TARGET_LEARNING_RATE*updated_quality\n",
    "        \n",
    "        batch_x.append(observation)\n",
    "        batch_y.append(desired_qualities)\n",
    "    \n",
    "    model.train(np.array(batch_x), np.array(batch_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b2900-e20f-4b14-9320-9fb6e28b7da6",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfd86bf-1649-461e-8276-d7ecf3f29674",
   "metadata": {},
   "outputs": [],
   "source": [
    "experience_memory = ExperienceMemory(memory_capacity=MAX_MEMORY_SIZE)\n",
    "\n",
    "model_1 = Model(observation_shape=env.observation_space.shape, action_shape=env.action_space.n, lr=LEARNING_RATE)\n",
    "model_2 = Model(observation_shape=env.observation_space.shape, action_shape=env.action_space.n, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "067bbbe4-157b-490b-9c4a-633caa221557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round:   1, reward:  26, experience size:  1026\n",
      "round:   2, reward:  15, experience size:  1041\n",
      "round:   3, reward:  21, experience size:  1062\n",
      "round:   4, reward:  22, experience size:  1084\n",
      "round:   5, reward:  10, experience size:  1094\n",
      "round:   6, reward:  14, experience size:  1108\n",
      "round:   7, reward:  10, experience size:  1118\n",
      "round:   8, reward:  16, experience size:  1134\n",
      "round:   9, reward:  16, experience size:  1150\n",
      "round:  10, reward:  10, experience size:  1160\n",
      "round:  11, reward:  16, experience size:  1176\n",
      "round:  12, reward:  11, experience size:  1187\n",
      "round:  13, reward:  30, experience size:  1217\n",
      "round:  14, reward:  16, experience size:  1233\n",
      "round:  15, reward:  11, experience size:  1244\n",
      "round:  16, reward:  11, experience size:  1255\n",
      "round:  17, reward:  11, experience size:  1266\n",
      "round:  18, reward:  41, experience size:  1307\n",
      "round:  19, reward:  24, experience size:  1331\n",
      "round:  20, reward:  19, experience size:  1350\n",
      "round:  21, reward:  16, experience size:  1366\n",
      "round:  22, reward:  19, experience size:  1385\n",
      "round:  23, reward:  20, experience size:  1405\n",
      "round:  24, reward:  23, experience size:  1428\n",
      "round:  25, reward:  45, experience size:  1473\n",
      "round:  26, reward:  14, experience size:  1487\n",
      "round:  27, reward:  20, experience size:  1507\n",
      "round:  28, reward:  21, experience size:  1528\n",
      "round:  29, reward:   9, experience size:  1537\n",
      "round:  30, reward:  21, experience size:  1558\n",
      "round:  31, reward:  23, experience size:  1581\n",
      "round:  32, reward:  32, experience size:  1613\n",
      "round:  33, reward:  24, experience size:  1637\n",
      "round:  34, reward:  33, experience size:  1670\n",
      "round:  35, reward:  32, experience size:  1702\n",
      "round:  36, reward:  52, experience size:  1754\n",
      "round:  37, reward:  53, experience size:  1807\n",
      "round:  38, reward:  19, experience size:  1826\n",
      "round:  39, reward:  13, experience size:  1839\n",
      "round:  40, reward:  13, experience size:  1852\n",
      "round:  41, reward:  28, experience size:  1880\n",
      "round:  42, reward:  25, experience size:  1905\n",
      "round:  43, reward:  13, experience size:  1918\n",
      "round:  44, reward:  27, experience size:  1945\n",
      "round:  45, reward:  17, experience size:  1962\n",
      "round:  46, reward:  19, experience size:  1981\n",
      "round:  47, reward:  20, experience size:  2001\n",
      "round:  48, reward:  30, experience size:  2031\n",
      "round:  49, reward:  16, experience size:  2047\n",
      "round:  50, reward:  44, experience size:  2091\n",
      "round:  51, reward:  18, experience size:  2109\n",
      "round:  52, reward:  26, experience size:  2135\n",
      "round:  53, reward:  25, experience size:  2160\n",
      "round:  54, reward:  25, experience size:  2185\n",
      "round:  55, reward:  14, experience size:  2199\n",
      "round:  56, reward:  28, experience size:  2227\n",
      "round:  57, reward:  20, experience size:  2247\n",
      "round:  58, reward:  11, experience size:  2258\n",
      "round:  59, reward:  13, experience size:  2271\n",
      "round:  60, reward:  37, experience size:  2308\n",
      "round:  61, reward:  20, experience size:  2328\n",
      "round:  62, reward:  12, experience size:  2340\n",
      "round:  63, reward:  22, experience size:  2362\n",
      "round:  64, reward:  10, experience size:  2372\n",
      "round:  65, reward:   9, experience size:  2381\n",
      "round:  66, reward:  20, experience size:  2401\n",
      "round:  67, reward:  17, experience size:  2418\n",
      "round:  68, reward:  16, experience size:  2434\n",
      "round:  69, reward:  20, experience size:  2454\n",
      "round:  70, reward:  15, experience size:  2469\n",
      "round:  71, reward:  17, experience size:  2486\n",
      "round:  72, reward:  17, experience size:  2503\n",
      "round:  73, reward:  15, experience size:  2518\n",
      "round:  74, reward:  11, experience size:  2529\n",
      "round:  75, reward:  10, experience size:  2539\n",
      "round:  76, reward:  36, experience size:  2575\n",
      "round:  77, reward:  12, experience size:  2587\n",
      "round:  78, reward:  49, experience size:  2636\n",
      "round:  79, reward:  10, experience size:  2646\n",
      "round:  80, reward:  17, experience size:  2663\n",
      "round:  81, reward:  22, experience size:  2685\n",
      "round:  82, reward:  17, experience size:  2702\n",
      "round:  83, reward:  13, experience size:  2715\n",
      "round:  84, reward:  18, experience size:  2733\n",
      "round:  85, reward:   9, experience size:  2742\n",
      "round:  86, reward:  15, experience size:  2757\n",
      "round:  87, reward:  14, experience size:  2771\n",
      "round:  88, reward:  10, experience size:  2781\n",
      "round:  89, reward:  11, experience size:  2792\n",
      "round:  90, reward:  23, experience size:  2815\n",
      "round:  91, reward:  22, experience size:  2837\n",
      "round:  92, reward:  13, experience size:  2850\n",
      "round:  93, reward:  13, experience size:  2863\n",
      "round:  94, reward:  10, experience size:  2873\n",
      "round:  95, reward:  10, experience size:  2883\n",
      "round:  96, reward:  18, experience size:  2901\n",
      "round:  97, reward:  14, experience size:  2915\n",
      "round:  98, reward:  18, experience size:  2933\n",
      "round:  99, reward:  32, experience size:  2965\n",
      "round: 100, reward:  18, experience size:  2983\n",
      "round: 101, reward:  10, experience size:  2993\n",
      "round: 102, reward:  14, experience size:  3007\n",
      "round: 103, reward:  14, experience size:  3021\n",
      "round: 104, reward:  11, experience size:  3032\n",
      "round: 105, reward:  11, experience size:  3043\n",
      "round: 106, reward:  11, experience size:  3054\n",
      "round: 107, reward:  12, experience size:  3066\n",
      "round: 108, reward:  12, experience size:  3078\n",
      "round: 109, reward:  12, experience size:  3090\n",
      "round: 110, reward:  15, experience size:  3105\n",
      "round: 111, reward:  11, experience size:  3116\n",
      "round: 112, reward:   9, experience size:  3125\n",
      "round: 113, reward:  18, experience size:  3143\n",
      "round: 114, reward:  12, experience size:  3155\n",
      "round: 115, reward:   8, experience size:  3163\n",
      "round: 116, reward:  14, experience size:  3177\n",
      "round: 117, reward:   8, experience size:  3185\n",
      "round: 118, reward:  10, experience size:  3195\n",
      "round: 119, reward:  14, experience size:  3209\n",
      "round: 120, reward:   9, experience size:  3218\n",
      "round: 121, reward:  13, experience size:  3231\n",
      "round: 122, reward:  11, experience size:  3242\n",
      "round: 123, reward:  25, experience size:  3267\n",
      "round: 124, reward:  11, experience size:  3278\n",
      "round: 125, reward:  13, experience size:  3291\n",
      "round: 126, reward:  30, experience size:  3321\n",
      "round: 127, reward:  14, experience size:  3335\n",
      "round: 128, reward:  14, experience size:  3349\n",
      "round: 129, reward:  10, experience size:  3359\n",
      "round: 130, reward:  11, experience size:  3370\n",
      "round: 131, reward:  13, experience size:  3383\n",
      "round: 132, reward:  14, experience size:  3397\n",
      "round: 133, reward:  11, experience size:  3408\n",
      "round: 134, reward:  17, experience size:  3425\n",
      "round: 135, reward:  10, experience size:  3435\n",
      "round: 136, reward:  13, experience size:  3448\n",
      "round: 137, reward:  12, experience size:  3460\n",
      "round: 138, reward:  15, experience size:  3475\n",
      "round: 139, reward:  15, experience size:  3490\n",
      "round: 140, reward:  17, experience size:  3507\n",
      "round: 141, reward:  21, experience size:  3528\n",
      "round: 142, reward:  13, experience size:  3541\n",
      "round: 143, reward:  12, experience size:  3553\n",
      "round: 144, reward:  15, experience size:  3568\n",
      "round: 145, reward:   9, experience size:  3577\n",
      "round: 146, reward:  11, experience size:  3588\n",
      "round: 147, reward:  24, experience size:  3612\n",
      "round: 148, reward:  10, experience size:  3622\n",
      "round: 149, reward:  11, experience size:  3633\n",
      "round: 150, reward:  12, experience size:  3645\n",
      "round: 151, reward:  11, experience size:  3656\n",
      "round: 152, reward:  15, experience size:  3671\n",
      "round: 153, reward:  14, experience size:  3685\n",
      "round: 154, reward:  10, experience size:  3695\n",
      "round: 155, reward:  15, experience size:  3710\n",
      "round: 156, reward:  14, experience size:  3724\n",
      "round: 157, reward:  13, experience size:  3737\n",
      "round: 158, reward:  10, experience size:  3747\n",
      "round: 159, reward:  15, experience size:  3762\n",
      "round: 160, reward:  17, experience size:  3779\n",
      "round: 161, reward:  11, experience size:  3790\n",
      "round: 162, reward:  10, experience size:  3800\n",
      "round: 163, reward:   9, experience size:  3809\n",
      "round: 164, reward:  12, experience size:  3821\n",
      "round: 165, reward:  11, experience size:  3832\n",
      "round: 166, reward:  18, experience size:  3850\n",
      "round: 167, reward:  16, experience size:  3866\n",
      "round: 168, reward:  13, experience size:  3879\n",
      "round: 169, reward:  10, experience size:  3889\n",
      "round: 170, reward:   8, experience size:  3897\n",
      "round: 171, reward:  16, experience size:  3913\n",
      "round: 172, reward:  16, experience size:  3929\n",
      "round: 173, reward:  19, experience size:  3948\n",
      "round: 174, reward:  12, experience size:  3960\n",
      "round: 175, reward:  11, experience size:  3971\n",
      "round: 176, reward:  10, experience size:  3981\n",
      "round: 177, reward:  14, experience size:  3995\n",
      "round: 178, reward:  18, experience size:  4013\n",
      "round: 179, reward:  19, experience size:  4032\n",
      "round: 180, reward:  25, experience size:  4057\n",
      "round: 181, reward:  16, experience size:  4073\n",
      "round: 182, reward:  11, experience size:  4084\n",
      "round: 183, reward:   9, experience size:  4093\n",
      "round: 184, reward:  11, experience size:  4104\n",
      "round: 185, reward:  44, experience size:  4148\n",
      "round: 186, reward:  21, experience size:  4169\n",
      "round: 187, reward:  29, experience size:  4198\n",
      "round: 188, reward:  11, experience size:  4209\n",
      "round: 189, reward:  21, experience size:  4230\n",
      "round: 190, reward:  13, experience size:  4243\n",
      "round: 191, reward:  14, experience size:  4257\n",
      "round: 192, reward:  40, experience size:  4297\n",
      "round: 193, reward:  20, experience size:  4317\n",
      "round: 194, reward:  13, experience size:  4330\n",
      "round: 195, reward:  15, experience size:  4345\n",
      "round: 196, reward:  27, experience size:  4372\n",
      "round: 197, reward:  14, experience size:  4386\n",
      "round: 198, reward:  15, experience size:  4401\n",
      "round: 199, reward:   9, experience size:  4410\n",
      "round: 200, reward:  13, experience size:  4423\n",
      "round: 201, reward:  16, experience size:  4439\n",
      "round: 202, reward:  14, experience size:  4453\n",
      "round: 203, reward:  21, experience size:  4474\n",
      "round: 204, reward:  12, experience size:  4486\n",
      "round: 205, reward:  18, experience size:  4504\n",
      "round: 206, reward:  24, experience size:  4528\n",
      "round: 207, reward:  18, experience size:  4546\n",
      "round: 208, reward:  21, experience size:  4567\n",
      "round: 209, reward:  32, experience size:  4599\n",
      "round: 210, reward:  44, experience size:  4643\n",
      "round: 211, reward:  46, experience size:  4689\n",
      "round: 212, reward:  27, experience size:  4716\n",
      "round: 213, reward:  29, experience size:  4745\n",
      "round: 214, reward:  50, experience size:  4795\n",
      "round: 215, reward:  70, experience size:  4865\n",
      "round: 216, reward:  19, experience size:  4884\n",
      "round: 217, reward:  30, experience size:  4914\n",
      "round: 218, reward:  24, experience size:  4938\n",
      "round: 219, reward:  23, experience size:  4961\n",
      "round: 220, reward:  19, experience size:  4980\n",
      "round: 221, reward:  56, experience size:  5000\n",
      "round: 222, reward:  30, experience size:  5000\n",
      "round: 223, reward:  46, experience size:  5000\n",
      "round: 224, reward:  26, experience size:  5000\n",
      "round: 225, reward:  31, experience size:  5000\n",
      "round: 226, reward:  96, experience size:  5000\n",
      "round: 227, reward:  75, experience size:  5000\n",
      "round: 228, reward:  61, experience size:  5000\n",
      "round: 229, reward:  75, experience size:  5000\n",
      "round: 230, reward:  29, experience size:  5000\n",
      "round: 231, reward:  32, experience size:  5000\n",
      "round: 232, reward:  31, experience size:  5000\n",
      "round: 233, reward:  32, experience size:  5000\n",
      "round: 234, reward:  66, experience size:  5000\n",
      "round: 235, reward:  52, experience size:  5000\n",
      "round: 236, reward:  43, experience size:  5000\n",
      "round: 237, reward:  45, experience size:  5000\n",
      "round: 238, reward:  30, experience size:  5000\n",
      "round: 239, reward:  49, experience size:  5000\n",
      "round: 240, reward:  39, experience size:  5000\n",
      "round: 241, reward:  67, experience size:  5000\n",
      "round: 242, reward:  86, experience size:  5000\n",
      "round: 243, reward:  62, experience size:  5000\n",
      "round: 244, reward:  71, experience size:  5000\n",
      "round: 245, reward:  96, experience size:  5000\n",
      "round: 246, reward: 121, experience size:  5000\n",
      "round: 247, reward:  83, experience size:  5000\n",
      "round: 248, reward: 122, experience size:  5000\n",
      "round: 249, reward:  94, experience size:  5000\n",
      "round: 250, reward: 108, experience size:  5000\n",
      "round: 251, reward:  60, experience size:  5000\n",
      "round: 252, reward: 127, experience size:  5000\n",
      "round: 253, reward: 293, experience size:  5000\n",
      "round: 254, reward:  89, experience size:  5000\n",
      "round: 255, reward: 151, experience size:  5000\n",
      "round: 256, reward: 112, experience size:  5000\n",
      "round: 257, reward:  95, experience size:  5000\n",
      "round: 258, reward: 137, experience size:  5000\n",
      "round: 259, reward: 177, experience size:  5000\n",
      "round: 260, reward: 177, experience size:  5000\n",
      "round: 261, reward: 110, experience size:  5000\n",
      "round: 262, reward: 106, experience size:  5000\n",
      "round: 263, reward:  82, experience size:  5000\n",
      "round: 264, reward:  81, experience size:  5000\n",
      "round: 265, reward:  94, experience size:  5000\n",
      "round: 266, reward:  94, experience size:  5000\n",
      "round: 267, reward:  96, experience size:  5000\n",
      "round: 268, reward: 111, experience size:  5000\n",
      "round: 269, reward: 139, experience size:  5000\n",
      "round: 270, reward:  86, experience size:  5000\n",
      "round: 271, reward: 133, experience size:  5000\n",
      "round: 272, reward:  84, experience size:  5000\n",
      "round: 273, reward: 133, experience size:  5000\n",
      "round: 274, reward:  90, experience size:  5000\n",
      "round: 275, reward: 130, experience size:  5000\n",
      "round: 276, reward:  71, experience size:  5000\n",
      "round: 277, reward:  78, experience size:  5000\n",
      "round: 278, reward:  88, experience size:  5000\n",
      "round: 279, reward: 200, experience size:  5000\n",
      "round: 280, reward: 176, experience size:  5000\n",
      "round: 281, reward: 138, experience size:  5000\n",
      "round: 282, reward: 117, experience size:  5000\n",
      "round: 283, reward: 111, experience size:  5000\n",
      "round: 284, reward: 208, experience size:  5000\n",
      "round: 285, reward:  69, experience size:  5000\n",
      "round: 286, reward: 105, experience size:  5000\n",
      "round: 287, reward: 114, experience size:  5000\n",
      "round: 288, reward: 170, experience size:  5000\n",
      "round: 289, reward: 145, experience size:  5000\n",
      "round: 290, reward: 120, experience size:  5000\n",
      "round: 291, reward:  98, experience size:  5000\n",
      "round: 292, reward:  81, experience size:  5000\n",
      "round: 293, reward: 144, experience size:  5000\n",
      "round: 294, reward: 232, experience size:  5000\n",
      "round: 295, reward: 136, experience size:  5000\n",
      "round: 296, reward: 142, experience size:  5000\n",
      "round: 297, reward: 133, experience size:  5000\n",
      "round: 298, reward: 500, experience size:  5000\n",
      "round: 299, reward: 179, experience size:  5000\n",
      "round: 300, reward: 142, experience size:  5000\n",
      "CPU times: user 10min 5s, sys: 16.6 s, total: 10min 22s\n",
      "Wall time: 9min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_step_count = 0\n",
    "eps = 1\n",
    "done = True\n",
    "\n",
    "# Filled the experience_memory with minimum number of experiences.\n",
    "while len(experience_memory) < MIN_MEMORY_SIZE:\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "    \n",
    "    if random.random() < 0.5:\n",
    "        e = simulate_experience(env, model_1, eps, observation)\n",
    "    else:\n",
    "        e = simulate_experience(env, model_2, eps, observation)\n",
    "    experience_memory.push(e)\n",
    "    \n",
    "    observation = e.new_observation\n",
    "    done = e.done\n",
    "\n",
    "# train the deep-q-model\n",
    "for curr_round in range(1, TRAIN_ROUNDS + 1):\n",
    "    observation = env.reset()\n",
    "    eps = get_epsilon(curr_round, TRAIN_ROUNDS)\n",
    "    cumulative_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # simulation to get new experiences\n",
    "        for sim_id in range(SIMULATIONS_PER_TRAINING):\n",
    "            if random.random() < 0.5:\n",
    "                e = simulate_experience(env, model_1, eps, observation)\n",
    "            else:\n",
    "                e = simulate_experience(env, model_2, eps, observation)\n",
    "            experience_memory.push(e)\n",
    "            \n",
    "            observation = e.new_observation\n",
    "            cumulative_reward += e.reward\n",
    "            done = e.done\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # train the main model\n",
    "        if len(experience_memory) >= MIN_MEMORY_SIZE:\n",
    "            train_step_count += 1\n",
    "            minibatch = experience_memory.sample(BATCH_SIZE)\n",
    "            if random.random() < 0.5:\n",
    "                model, target_model = model_1, model_2\n",
    "            else:\n",
    "                model, target_model = model_2, model_1\n",
    "            train(model, target_model, minibatch)\n",
    "        \n",
    "    print(f'round: {curr_round:3}, reward: {int(cumulative_reward):3}, experience size: {len(experience_memory):5}')\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
